---
title: End-to-end tutorials in Microsoft Fabric
description: This article lists the end-to-end tutorials in Microsoft Fabric. They walk you through a scenario, starting from data acquisition to data consumption and help you with a foundational understanding of Fabric.
ms.reviewer: sngun
ms.author: sngun
author: SnehaGunda
ms.topic: conceptual
ms.date: 5/23/2023
---

# End-to-end tutorials in Microsoft Fabric

In this article, you'll find a comprehensive list of end-to-end tutorials available in Microsoft Fabric. These tutorials will guide you through a scenario that covers the entire process, from data acquisition to data consumption. They're designed to help you develop a foundational understanding of the Fabric UI, the various experiences supported by Fabric, their integration points, and the professional and citizen developer experiences that are available.

[!INCLUDE [preview-note](../includes/preview-note.md)]

## Tutorials

|Tutorial name  |Scenario |
|---------|---------|
|[Lakehouse](../data-engineering/tutorial-lakehouse-introduction.md) | In this tutorial, you'll ingest, transform, and load the data of a fictional retail company, Wide World Importers, into the lakehouse and analyze sales data across various dimensions.  |
|[Data Science](../data-science/tutorial-data-science-introduction.md)    |  In this tutorial, you'll explore, clean, and transform a taxicab trip dataset, and build a machine learning model to predict trip duration at scale on a large dataset.   |
|[Real-Time Analytics](../real-time-analytics/tutorial-introduction.md)   | In this tutorial, you'll use the streaming and query capabilities of Real-Time Analytics to analyze the New York Yellow Taxi trip dataset. You'll uncover essential insights into trip statistics, taxi demand across the boroughs of New York, and other related insights. |
|[Data warehouse](../placeholder.md) |  In this tutorial, you'll build an end-to-end data warehouse for the fictional Wide World Importers company. You'll ingest data into data warehouse, transform it using T-SQL and pipelines, run queries, and build reports. |
|[Data Factory](../data-factory/tutorial-end-to-end-introduction.md) | In this tutorial, you'll create a complete data integration scenario using a pipeline and dataflow. |

## Next steps

* [Create a workspace](create-workspaces.md)
* Discover data items in the [OneLake data hub](onelake-data-hub.md)