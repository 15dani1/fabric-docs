### YamlMime:FAQ
metadata:
  title: Spark workspace administration settings FAQ
  description: Answers to frequently asked questions about Spark workspace administration settings.
  author: santhoshravindran7
  ms.author: saravi
  ms.topic: faq
  ms.date: 02/24/2023
title: Spark workspace administration settings FAQ
summary: |
  This article lists answers to frequently asked questions about Spark workspace administration settings.

sections:
  - name: Workspace administration settings
    questions:
      - question: How do I use the RBAC roles to configure my Spark workspace settings? 
        answer: Use the **Manage Access** menu to add **Admin** permissions for specific users, distribution groups, or security groups. You can also use this menu to make changes to the workspace and to grant access to add, modify, or delete the Spark workspace settings.

      - question: Do the changes made at the workspace level for Spark properties affect active notebook sessions or scheduled Spark jobs?
        answer: When you make a configuration change at the workspace level, it doesn't affect active Spark sessions (batch or notebook based). You must start a new notebook or batch session after the saving the new configuration settings for the settings to take effect.

      - question: Can I configure the Node Family, Spark Runtime, and Spark Properties at a Capacity level?
        answer: Currently, Spark Administration settings are only available at the workspace level.

      - question: Can I choose different node families for different notebooks and Spark job definitions in my workspace?
        answer: Currently, you can only select either a Memory Optimized (Default) or a Hardware Accelerated GPU based node family for the entire workspace. You aren't able to select Hardware Accelerated GPU based pools for specific notebooks in a workspace. You must create a new workspace or modify node family settings for the current workspace  to “Hardware Accelerated GPU” based pools for compute intensive workloads.

      - question: What versions of Spark are supported?
        answer: Currently, Spark version 3.2 is the only supported version. In the upcoming release, more versions of Spark will be available.

      - question: Can I configure these settings at a notebook level?
        answer: Currently, the Spark Administration settings are only available at the workspace level.

      - question: Can I configure the minimum and maximum number of nodes for the selected node family?
        answer: Currently, node limit configuration isn't available. This capability will be enabled in future releases.

      - question: Can I enable Autoscaling for the Spark Pools in a memory optimized or hardware accelerated GPU based Node Family?
        answer: Autoscaling isn't currently available. This capability will be enabled in future releases.

      - question: Is Intelligent Caching for the Spark Pools supported or enabled by default for a workspace?
        answer: Intelligent Caching is enabled by default for the Spark pools for all workspaces.
