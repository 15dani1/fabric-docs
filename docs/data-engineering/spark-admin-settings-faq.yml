### YamlMime:FAQ
metadata:
  title: Apache Spark workspace administration settings FAQ
  description: This article lists answers to frequently asked questions about Apache Spark workspace administration settings.
  author: santhoshravindran7
  ms.author: saravi
  ms.topic: faq
  ms.custom:
    - ignite-2023
  ms.date: 05/23/2023
title: Apache Spark workspace administration settings FAQ
summary: |
  This article lists answers to frequently asked questions about Apache Spark workspace administration settings.

sections:
  - name: Workspace administration settings
    questions:
      - question: How do I use the RBAC roles to configure my Spark workspace settings? 
        answer: Use the **Manage Access** menu to add **Admin** permissions for specific users, distribution groups, or security groups. You can also use this menu to make changes to the workspace and to grant access to add, modify, or delete the Spark workspace settings.

      - question: Are the changes made to the Spark properties at the workspace level apply to the active notebook sessions or scheduled Spark jobs?
        answer: When you make a configuration change at the workspace level, it's not applied to active Spark sessions. This includes batch or notebook based sessions. You must start a new notebook or a batch session after saving the new configuration settings for the settings to take effect.

      - question: Can I configure the node family, Spark runtime, and Spark properties at a capacity level?
        answer: Yes, you can change the runtime, or manage the spark properties using the Data Engineering/Science settings as part of the capacity admin settings page. You need have the capacity admin access to view and change these capacity settings.

      - question: Can I choose different node families for different notebooks and Spark job definitions in my workspace?
        answer: Currently, you can only select Memory Optimized based node family for the entire workspace. 

      - question: What versions of Spark are supported?
        answer: Currently, Spark version 3.3 is the only supported version. Additional versions will be available in the upcoming release.

      - question: Can I configure these settings at a notebook level?
        answer: Currently, the Spark administration settings are only available at the workspace and capacity level

      - question: Can I configure the minimum and maximum number of nodes for the selected node family?
        answer: Currently, node limit configuration isn't available. This capability will be enabled in future releases.

      - question: Can I enable Autoscaling for the Spark Pools in a memory optimized or hardware accelerated GPU based node family?
        answer: Autoscaling isn't currently available. This capability will be enabled in future releases.

      - question: Is Intelligent Caching for the Spark Pools supported or enabled by default for a workspace?
        answer: Intelligent Caching is enabled by default for the Spark pools for all workspaces.
